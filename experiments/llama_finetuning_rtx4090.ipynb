{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f78d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a8c0a3-69cd-4659-b885-a86e68912c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF:\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caefefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VRAM MONITORING UTILITIES\n",
    "# ==============================================================================\n",
    "class VRAMMonitor:\n",
    "    \"\"\"Monitor and log VRAM usage\"\"\"\n",
    "    \n",
    "    # Get VRAM Usage Method\n",
    "    @staticmethod\n",
    "    def get_vram_usage():\n",
    "        \"\"\"Get current VRAM Information\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return {\"allocated\": 0, \"reserved\": 0, \"free\": 0, \"total\": 0}\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3 # GB\n",
    "        free = total - allocated # GB\n",
    "        \n",
    "        return {\n",
    "            \"allocated_gb\": round(allocated, 2),\n",
    "            \"reserved_gb\": round(reserved, 2),\n",
    "            \"free_gb\": round(free, 2),\n",
    "            \"total_gb\": round(total, 2),\n",
    "            \"utilization_pct\": round((allocated / total) * 100, 2)\n",
    "        }\n",
    "        \n",
    "    # Get VRAM Usage Status\n",
    "    @staticmethod\n",
    "    def print_vram_status(step=None, prefix=\"\"):\n",
    "        \"\"\"Print VRAM Status\"\"\"\n",
    "        stats = VRAMMonitor.get_vram_usage()\n",
    "        step_info = f\"[STEP {step}] \" if step is not None else \"\"\n",
    "        \n",
    "        print(f\"{prefix}{step_info}VRAM: {stats['allocated_gb']:.2f}GB / {stats['total_gb']:.2f}GB \"\n",
    "              f\"({stats['utilization_pct']:.1f}%) | RESERVED: {stats['reserved_gb']:.2f}GB\")\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b768b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOGGING CALLBACK\n",
    "# ==============================================================================\n",
    "class DetailedLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for more detailed logging\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir, log_every_n_steps=10):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        \n",
    "        # Initialize log files\n",
    "        self.metrics_file = self.log_dir / \"training_metrics.jsonl\"\n",
    "        self.vram_file = self.log_dir / \"vram_usage.jsonl\"\n",
    "        self.summary_file = self.log_dir / \"training_summary.json\"\n",
    "        \n",
    "        self.training_start_time = None\n",
    "        self.all_metrics = []\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log the start of training\"\"\"\n",
    "        self.training_start_time = datetime.now()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        print(\"TRAINING STARTED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"START TIME: {self.training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"LOG DIRECTORY: {self.log_dir}\")\n",
    "        \n",
    "        initial_vram = VRAMMonitor.print_vram_status(prefix=\"INITIAL \")\n",
    "        \n",
    "        # Saving begin information\n",
    "        summary = {\n",
    "            \"start_time\": self.training_start_time.isoformat(),\n",
    "            \"total_steps\": state.max_steps,\n",
    "            \"initial_vram\": initial_vram\n",
    "        }\n",
    "        \n",
    "        with open(self.summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log after each step\"\"\"\n",
    "        if state.global_step % self.log_every_n_steps == 0:\n",
    "            # Get VRAM stats\n",
    "            vram_stats = VRAMMonitor.get_vram_usage()\n",
    "            vram_stats[\"step\"] = state.global_step\n",
    "            vram_stats[\"timestamp\"] = datetime.now().isoformat()\n",
    "            \n",
    "            # Write VRAM log\n",
    "            with open(self.vram_file, 'a') as f:\n",
    "                f.write(json.dumps(vram_stats) + \"\\n\")\n",
    "            \n",
    "            # Print console\n",
    "            VRAMMonitor.print_vram_status(step=state.global_step)\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Record metrics\"\"\"\n",
    "        if logs:\n",
    "            log_entry = {\n",
    "                \"step\": state.global_step,\n",
    "                \"epoch\": state.epoch,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                **logs\n",
    "            }\n",
    "            \n",
    "            # Saving in file\n",
    "            with open(self.metrics_file, 'a') as f:\n",
    "                f.write(json.dumps(log_entry) + \"\\n\")\n",
    "            \n",
    "            self.all_metrics.append(log_entry)\n",
    "            \n",
    "            # Print to console in a nice format\n",
    "            if \"loss\" in logs:\n",
    "                print(f\"STEP {state.global_step} | LOSS: {logs['loss']:.4f} | \"\n",
    "                      f\"LR: {logs.get('learning_rate', 0):.2e}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Log the training session at the end\"\"\"\n",
    "        training_end_time = datetime.now()\n",
    "        duration = training_end_time - self.training_start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"END TIME: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"DURATION: {duration}\")\n",
    "        \n",
    "        final_vram = VRAMMonitor.print_vram_status(prefix=\"Final \")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if self.all_metrics:\n",
    "            losses = [m[\"loss\"] for m in self.all_metrics if \"loss\" in m]\n",
    "            if losses:\n",
    "                avg_loss = sum(losses) / len(losses)\n",
    "                min_loss = min(losses)\n",
    "                final_loss = losses[-1]\n",
    "                \n",
    "                print(f\"\\nTRAINING STATISTICS:\")\n",
    "                print(f\"=> AVERAGE LOSS: {avg_loss:.4f}\")\n",
    "                print(f\"=>MINIMUM LOSS: {min_loss:.4f}\")\n",
    "                print(f\"=>FINAL LOSS: {final_loss:.4f}\")\n",
    "        \n",
    "        # Update summary\n",
    "        try:\n",
    "            with open(self.summary_file, 'r') as f:\n",
    "                summary = json.load(f)\n",
    "        except:\n",
    "            summary = {}\n",
    "        \n",
    "        summary.update({\n",
    "            \"end_time\": training_end_time.isoformat(),\n",
    "            \"duration_seconds\": duration.total_seconds(),\n",
    "            \"total_steps_completed\": state.global_step,\n",
    "            \"final_vram\": final_vram,\n",
    "            \"final_metrics\": self.all_metrics[-1] if self.all_metrics else {}\n",
    "        })\n",
    "        \n",
    "        with open(self.summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nLOGS SAVED TO: {self.log_dir}\")\n",
    "        print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bfa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import gc\n",
    "\n",
    "class HoloAdam(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Memory-Optimized Holo-Gradient Optimizer\n",
    "    - Chunked FFT/IFFT to prevent OOM\n",
    "    - Aggressive memory cleanup\n",
    "    - Gradient checkpointing support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params,\n",
    "                 lr=1e-4,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-8,\n",
    "                 weight_decay=0.01,\n",
    "                 holo_ratio=20,\n",
    "                 v_block_size=32, \n",
    "                 heads=2, # Change from 4 -> 2\n",
    "                 min_compress_size=8192, \n",
    "                 resonance_factor=0.4, \n",
    "                 warmup_steps=1000,\n",
    "                 codebook_size=256, \n",
    "                 codebook_width=8192,\n",
    "                 fft_chunk_size=2_000_000  # Reduced default for safety\n",
    "                ):\n",
    "        \n",
    "        defaults = dict(\n",
    "            lr=lr, \n",
    "            betas=betas, \n",
    "            eps=eps, \n",
    "            weight_decay=weight_decay,\n",
    "            ratio=holo_ratio, \n",
    "            v_block=v_block_size, \n",
    "            heads=heads,\n",
    "            min_size=min_compress_size, \n",
    "            max_rf=resonance_factor,\n",
    "            warmup=warmup_steps, \n",
    "            cb_size=codebook_size, \n",
    "            cb_width=codebook_width,\n",
    "            fft_chunk_size=fft_chunk_size\n",
    "        )\n",
    "        \n",
    "        super(HoloAdam, self).__init__(params, defaults)\n",
    "\n",
    "        # Keep codebook on CPU to save GPU memory\n",
    "        g_cpu = torch.Generator()\n",
    "        g_cpu.manual_seed(42)\n",
    "        phases = torch.rand(codebook_size, codebook_width, generator=g_cpu) * 2 * math.pi\n",
    "        self.keys_codebook = torch.polar(torch.ones_like(phases), phases)\n",
    "        self.dev_keys = None\n",
    "\n",
    "    def _get_keys(self, device, dim, idx):\n",
    "        \"\"\"\n",
    "        Lazy transfer keys to device only when needed\n",
    "        \"\"\"\n",
    "        if self.dev_keys is None or self.dev_keys.device != device:\n",
    "            # Clear old device keys if exists\n",
    "            if self.dev_keys is not None:\n",
    "                del self.dev_keys\n",
    "                torch.cuda.empty_cache()\n",
    "            self.dev_keys = self.keys_codebook.to(device)\n",
    "        \n",
    "        sel = self.dev_keys[idx]\n",
    "        \n",
    "        if dim <= self.dev_keys.shape[1]: \n",
    "            return sel[:, :dim]\n",
    "        \n",
    "        repeats = (dim // self.dev_keys.shape[1]) + 1\n",
    "        return sel.repeat(1, repeats)[:, :dim]\n",
    "\n",
    "    def _get_annealed_rf(self, max_rf, step, warmup):\n",
    "        \"\"\"Annealed resonance factor\"\"\"\n",
    "        if step < warmup:\n",
    "            return max_rf * (step / warmup)\n",
    "        return max_rf\n",
    "\n",
    "    def _chunked_fft_compress(self, grad_flat, h_dim, keys, chunk_size):\n",
    "        \"\"\"\n",
    "        Memory-efficient chunked FFT compression\n",
    "        \n",
    "        Critical optimizations:\n",
    "        - Process small batches at a time\n",
    "        - Immediate memory cleanup after each batch\n",
    "        - No intermediate tensor accumulation\n",
    "        \"\"\"\n",
    "        total_length = grad_flat.shape[0]\n",
    "        n_full_chunks = total_length // h_dim\n",
    "        \n",
    "        # Calculate safe batch size\n",
    "        chunks_per_batch = max(1, chunk_size // h_dim)\n",
    "        \n",
    "        # Pre-allocate result buffer\n",
    "        compressed = torch.zeros(h_dim, dtype=torch.complex64, device=grad_flat.device)\n",
    "        normalization = 1.0 / math.sqrt(max(1, n_full_chunks))\n",
    "        \n",
    "        # Process in small batches\n",
    "        for batch_start in range(0, n_full_chunks, chunks_per_batch):\n",
    "            batch_end = min(batch_start + chunks_per_batch, n_full_chunks)\n",
    "            batch_size = batch_end - batch_start\n",
    "            \n",
    "            # Extract batch - avoid creating large intermediate tensors\n",
    "            start_idx = batch_start * h_dim\n",
    "            end_idx = batch_end * h_dim\n",
    "            \n",
    "            # Reshape in-place view (no copy)\n",
    "            batch_grad = grad_flat[start_idx:end_idx].view(batch_size, h_dim)\n",
    "            \n",
    "            # Get corresponding keys slice\n",
    "            batch_keys = keys[batch_start:batch_end]\n",
    "            \n",
    "            # FFT on small batch\n",
    "            with torch.amp.autocast(enabled=False):  # Disable AMP for FFT\n",
    "                batch_fft = torch.fft.fft(batch_grad.to(torch.float32), dim=-1)\n",
    "            \n",
    "            # Accumulate compressed result\n",
    "            compressed.add_((batch_fft * batch_keys).sum(dim=0) * normalization)\n",
    "            \n",
    "            # Critical: Delete immediately to free memory\n",
    "            del batch_grad, batch_keys, batch_fft\n",
    "        \n",
    "        return compressed\n",
    "\n",
    "    def _chunked_ifft_decompress(self, compressed, h_dim, keys, original_length, chunk_size):\n",
    "        \"\"\"\n",
    "        Memory-efficient chunked IFFT decompression\n",
    "        \"\"\"\n",
    "        n_full_chunks = original_length // h_dim\n",
    "        chunks_per_batch = max(1, chunk_size // h_dim)\n",
    "        \n",
    "        # Pre-allocate result\n",
    "        result = torch.zeros(original_length, dtype=torch.float32, device=compressed.device)\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, n_full_chunks, chunks_per_batch):\n",
    "            batch_end = min(batch_start + chunks_per_batch, n_full_chunks)\n",
    "            batch_size = batch_end - batch_start\n",
    "            \n",
    "            # Get keys for this batch\n",
    "            batch_keys = keys[batch_start:batch_end]\n",
    "            \n",
    "            # Decompress: broadcast and multiply with conjugate\n",
    "            with torch.amp.autocast(enabled=False):\n",
    "                batch_freq = compressed.unsqueeze(0).expand(batch_size, -1) * torch.conj(batch_keys)\n",
    "                batch_spatial = torch.fft.ifft(batch_freq, dim=-1).real\n",
    "            \n",
    "            # Write to result buffer\n",
    "            start_idx = batch_start * h_dim\n",
    "            end_idx = batch_end * h_dim\n",
    "            result[start_idx:end_idx] = batch_spatial.flatten()\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del batch_keys, batch_freq, batch_spatial\n",
    "        \n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Optimizer step with aggressive memory management\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps, wd = group['eps'], group['weight_decay']\n",
    "            num_heads = group['heads']\n",
    "            fft_chunk_size = group['fft_chunk_size']\n",
    "            \n",
    "            for p in group['params']:\n",
    "                if p.grad is None: \n",
    "                    continue\n",
    "                \n",
    "                # CRITICAL: Work on a detached copy to avoid autograd overhead\n",
    "                grad = p.grad.detach().to(torch.float32)\n",
    "                \n",
    "                # Weight decay (decoupled)\n",
    "                if wd != 0:\n",
    "                    p.data.mul_(1 - lr * wd)\n",
    "\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # Determine compression strategy\n",
    "                is_large = p.numel() > group['min_size']\n",
    "                is_matrix = p.dim() > 1\n",
    "                \n",
    "                should_compress = is_large and is_matrix\n",
    "                \n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    \n",
    "                    if should_compress:\n",
    "                        h = max(64, p.numel() // group['ratio'])\n",
    "                        if h % 2 != 0: \n",
    "                            h += 1\n",
    "                        \n",
    "                        pad = (h - (p.numel() % h)) % h\n",
    "                        n_ch = (p.numel() + pad) // h\n",
    "                        \n",
    "                        state['idx'] = (torch.arange(n_ch, device=p.device) % group['cb_size'])\n",
    "                        state['hm'] = torch.zeros(num_heads, h, dtype=torch.complex64, device=p.device)\n",
    "                        state['h_dim'] = h\n",
    "                        state['sv'] = torch.zeros(\n",
    "                            math.ceil(p.numel() / group['v_block']), \n",
    "                            dtype=torch.float32, \n",
    "                            device=p.device\n",
    "                        )\n",
    "                        state['mode'] = 'holo'\n",
    "                    else:\n",
    "                        state['m'] = torch.zeros_like(p, dtype=torch.float32)\n",
    "                        state['v'] = torch.zeros_like(p, dtype=torch.float32)\n",
    "                        state['mode'] = 'dense'\n",
    "\n",
    "                state['step'] += 1\n",
    "                \n",
    "                # ============================================================\n",
    "                # DENSE MODE - Standard Adam\n",
    "                # ============================================================\n",
    "                if state['mode'] == 'dense':\n",
    "                    m, v = state['m'], state['v']\n",
    "                    \n",
    "                    m.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                    v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                    \n",
    "                    denom = v.sqrt().add_(eps)\n",
    "                    step_size = lr * math.sqrt(1 - beta2 ** state['step']) / (1 - beta1 ** state['step'])\n",
    "                    \n",
    "                    p.data.addcdiv_(m, denom, value=-step_size)\n",
    "                    \n",
    "                    del grad  # Cleanup\n",
    "                    continue\n",
    "\n",
    "                # ============================================================\n",
    "                # HOLOGRAPHIC MODE\n",
    "                # ============================================================\n",
    "                hm_heads = state['hm']\n",
    "                sv = state['sv']\n",
    "                h_dim = state['h_dim']\n",
    "                \n",
    "                # --- Update Second Moment (Variance) ---\n",
    "                g_sq = grad.square().flatten()\n",
    "                pad_v = (group['v_block'] - (g_sq.numel() % group['v_block'])) % group['v_block']\n",
    "                if pad_v: \n",
    "                    g_sq = F.pad(g_sq, (0, pad_v))\n",
    "                \n",
    "                sv.mul_(beta2).add_(g_sq.view(-1, group['v_block']).mean(1), alpha=1 - beta2)\n",
    "                del g_sq  # Free immediately\n",
    "                \n",
    "                # --- Prepare Gradient for FFT ---\n",
    "                g_flat = grad.flatten()\n",
    "                pad_m = (h_dim - (g_flat.numel() % h_dim)) % h_dim\n",
    "                if pad_m: \n",
    "                    g_flat = F.pad(g_flat, (0, pad_m))\n",
    "                \n",
    "                original_length = g_flat.numel()\n",
    "                \n",
    "                # Pre-allocate accumulator\n",
    "                m_accum = torch.zeros_like(grad)\n",
    "                \n",
    "                # --- Process Each Head ---\n",
    "                for h_idx in range(num_heads):\n",
    "                    # Generate indices for this head\n",
    "                    head_indices = (state['idx'] + h_idx * 13) % group['cb_size']\n",
    "                    \n",
    "                    # Get keys (reuse if possible)\n",
    "                    keys_full = self._get_keys(p.device, h_dim, head_indices)\n",
    "                    \n",
    "                    # === COMPRESS with Chunked FFT ===\n",
    "                    compressed = self._chunked_fft_compress(\n",
    "                        g_flat, h_dim, keys_full, fft_chunk_size\n",
    "                    )\n",
    "                    \n",
    "                    # Update hologram state\n",
    "                    hm_heads[h_idx].mul_(beta1).add_(compressed, alpha=1 - beta1)\n",
    "                    del compressed\n",
    "                    \n",
    "                    # === DECOMPRESS with Chunked IFFT ===\n",
    "                    decompressed = self._chunked_ifft_decompress(\n",
    "                        hm_heads[h_idx], h_dim, keys_full, original_length, fft_chunk_size\n",
    "                    )\n",
    "                    \n",
    "                    # Remove padding\n",
    "                    if pad_m:\n",
    "                        decompressed = decompressed[:-pad_m]\n",
    "                    \n",
    "                    # Accumulate\n",
    "                    m_accum.add_(decompressed.view_as(grad))\n",
    "                    \n",
    "                    # Critical cleanup\n",
    "                    del decompressed\n",
    "                    if h_idx < num_heads - 1:  # Don't delete on last iteration if reusing\n",
    "                        del keys_full\n",
    "\n",
    "                # Average across heads\n",
    "                m_accum.div_(num_heads)\n",
    "                \n",
    "                # --- Resonance Gating ---\n",
    "                curr_rf = self._get_annealed_rf(group['max_rf'], state['step'], group['warmup'])\n",
    "                \n",
    "                # Memory-efficient gating\n",
    "                gate = grad.mul(m_accum).mul_(5.0).tanh_().mul_(0.5).add_(0.5)\n",
    "                m_gated = m_accum.mul_(1.0 - curr_rf).add_(m_accum.mul(gate).mul_(curr_rf))\n",
    "                \n",
    "                del gate, m_accum  # Free before next allocation\n",
    "                \n",
    "                # --- Build Denominator ---\n",
    "                v_exp = sv.repeat_interleave(group['v_block'])\n",
    "                if pad_v: \n",
    "                    v_exp = v_exp[:-pad_v]\n",
    "                denom = v_exp.view_as(grad).sqrt_().add_(eps)\n",
    "                del v_exp\n",
    "                \n",
    "                # --- Apply Update ---\n",
    "                step_size = lr * math.sqrt(1 - beta2 ** state['step']) / (1 - beta1 ** state['step'])\n",
    "                p.data.addcdiv_(m_gated.clamp_(-2.0, 2.0), denom, value=-step_size)\n",
    "                \n",
    "                # Final cleanup for this parameter\n",
    "                del grad, g_flat, m_gated, denom\n",
    "            \n",
    "            # Aggressive memory cleanup after each parameter group\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff2d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AUTO CONFIGURATION\n",
    "# ==============================================================================\n",
    "def auto_configure_training(target_vram_gb=None):\n",
    "    \"\"\"Automatic training configuration based on VRAM\"\"\"\n",
    "    if target_vram_gb is None:\n",
    "        if torch.cuda.is_available():\n",
    "            target_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        else:\n",
    "            target_vram_gb = 0\n",
    "            print(\"âš ï¸ WARNING: GPU NOT FOUND!\")\n",
    "    \n",
    "    print(f\">>> DETECT GPU VRAM: {target_vram_gb:.2f} GB\")\n",
    "    \n",
    "    TARGET_EFFECTIVE_BS = 32\n",
    "    \n",
    "    if target_vram_gb < 16:\n",
    "        print(f'>>> ENABLING ENTRY-LEVEL MODE (T4/16GB) - MAXIMIZES SAVINGS')\n",
    "        model_id = \"Qwen/Qwen2.5-1.5B\"\n",
    "        batch_size = 1\n",
    "        holo_ratio = 64\n",
    "    elif target_vram_gb < 25:\n",
    "        print(f'>>> ENABLING CONSUMER MODE (RTX3090/4090) - 24GB VRAM - BALANCED')\n",
    "        model_id = \"Qwen/Qwen2.5-3B\"\n",
    "        batch_size = 2\n",
    "        holo_ratio = 50\n",
    "    elif target_vram_gb < 50:\n",
    "        print(f'>>> ENABLING WORKSTATION MODE (L40S) - HIGH PERFORMANCE')\n",
    "        model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "        batch_size = 2\n",
    "        holo_ratio = 64\n",
    "    else:\n",
    "        print(f'>>> ENABLING DATA CENTER MODE (A100) - EXTREME PERFORMANCE')\n",
    "        model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "        batch_size = 16\n",
    "        holo_ratio = 8\n",
    "    \n",
    "    grad_accum = max(1, TARGET_EFFECTIVE_BS // batch_size)\n",
    "    \n",
    "    config = {\n",
    "        \"model_id\": model_id,\n",
    "        \"per_device_train_batch_size\": batch_size,\n",
    "        \"gradient_accumulation_steps\": grad_accum,\n",
    "        \"holo_ratio\": holo_ratio\n",
    "    }\n",
    "    \n",
    "    print(f'# MODEL: {model_id}')\n",
    "    print(f'# BATCH SIZE PER DEVICE: {batch_size}')\n",
    "    print(f\"# ACCUMULATION GRADIENT: {grad_accum}\")\n",
    "    print(f'# HOLO COMPRESSION RATIO: {holo_ratio}')\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9135a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HOLO TRAINER\n",
    "# ==============================================================================\n",
    "class HoloTrainer(Trainer):\n",
    "    \"\"\"Enhanced Trainer with HoloAdam optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, holo_ratio=20, resonance_factor=0.4, **kwargs):\n",
    "        self.holo_ratio = holo_ratio\n",
    "        self.resonance_factor = resonance_factor\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = HoloAdam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "            holo_ratio=self.holo_ratio,\n",
    "            resonance_factor=self.resonance_factor,\n",
    "        )\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8909dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADAFACTOR TRAINER\n",
    "# ==============================================================================\n",
    "class AdafactorTrainer(Trainer):\n",
    "    \"\"\"Enhanced Trainer with Adafactor optimizer\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adafactor(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "        )\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb40380-ee41-4308-906f-1af5eb4d765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HOLO TRAINING FUNCTION\n",
    "# ==============================================================================\n",
    "def holo_training():\n",
    "    # Auto Configuration\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INIT TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    config = auto_configure_training()\n",
    "    \n",
    "    # Configure parameters\n",
    "    MODEL_ID = config[\"model_id\"]\n",
    "    BATCH_SIZE = config[\"per_device_train_batch_size\"]\n",
    "    GRADIENT_ACCUMULATION_STEPS = config[\"gradient_accumulation_steps\"]\n",
    "    HOLO_RATIO = config[\"holo_ratio\"]\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    LR = 2e-5\n",
    "    RESONANCE_FACTOR = 0.4\n",
    "    NUM_TRAIN_EPOCHS = 1\n",
    "    LOGGING_STEPS = 5\n",
    "    SAVE_STEPS = 100\n",
    "    \n",
    "    # Creating folder output with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_DIR = f\"./holo_llm_checkpoints_{timestamp}\"\n",
    "    LOG_DIR = f\"{OUTPUT_DIR}/logs\"\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“ OUTPUT DIRECTORY: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸ“ LOG DIRECTORY: {LOG_DIR}\")\n",
    "    \n",
    "    # Loading model and tokenizer\n",
    "    print(\"\\n>>> LOADING MODEL AND TOKENIZER...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=\"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\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        use_cache=False,\n",
    "        token=\"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\"\n",
    "    ).cuda()\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    VRAMMonitor.print_vram_status(prefix=\"After Model Load: \")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\n>>> LOADING DATASET...\")\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:2000]\")\n",
    "    \n",
    "    def fmt(x):\n",
    "        return {\"text\": f\"User: {x['instruction']}\\nAI: {x['output']}\"}\n",
    "    \n",
    "    dataset = dataset.map(fmt)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        outputs[\"labels\"] = [ids.copy() for ids in outputs[\"input_ids\"]]\n",
    "        return outputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Setup training arguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_dir=LOG_DIR,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        \n",
    "        save_strategy=\"no\",\n",
    "        # save_steps=SAVE_STEPS,\n",
    "        # save_total_limit=3,\n",
    "        \n",
    "        report_to=\"tensorboard\",\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "    \n",
    "    # Init callback\n",
    "    logging_callback = DetailedLoggingCallback(\n",
    "        log_dir=LOG_DIR,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Init HoloAdam trainer\n",
    "    trainer = HoloTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        holo_ratio=HOLO_RATIO,\n",
    "        resonance_factor=RESONANCE_FACTOR,\n",
    "        callbacks=[logging_callback]\n",
    "    )\n",
    "    \n",
    "    # Saving config\n",
    "    config_file = Path(OUTPUT_DIR) / \"training_config.json\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"model_id\": MODEL_ID,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"holo_ratio\": HOLO_RATIO,\n",
    "            \"learning_rate\": LR,\n",
    "            \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "            \"num_epochs\": NUM_TRAIN_EPOCHS,\n",
    "            \"resonance_factor\": RESONANCE_FACTOR,\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ TRAINING CONFIG SAVED TO: {config_file}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"START TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Saving model\n",
    "    final_model_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "    print(f\"\\nðŸ’¾ Saving the final model now: {final_model_dir}\")\n",
    "    trainer.save_model(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "    \n",
    "    print(\"\\nâœ… TRAINING SUCESSFULLY!\")\n",
    "    print(f\"ðŸ“Š LOGS: {LOG_DIR}\")\n",
    "    print(f\"ðŸ’¾ CHECKPOINTS: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸŽ¯ FINAL MODEL: {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c015965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INIT TRAINING PIPELINE\n",
      "================================================================================\n",
      ">>> DETECT GPU VRAM: 47.38 GB\n",
      ">>> ENABLING WORKSTATION MODE (L40S) - HIGH PERFORMANCE\n",
      "# MODEL: meta-llama/Meta-Llama-3-8B\n",
      "# BATCH SIZE PER DEVICE: 2\n",
      "# ACCUMULATION GRADIENT: 16\n",
      "# HOLO COMPRESSION RATIO: 64\n",
      "\n",
      "ðŸ“ OUTPUT DIRECTORY: ./holo_llm_checkpoints_20260118_131216\n",
      "ðŸ“ LOG DIRECTORY: ./holo_llm_checkpoints_20260118_131216/logs\n",
      "\n",
      ">>> LOADING MODEL AND TOKENIZER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9535405602431dae0b57c75321cfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Model Load: VRAM: 14.96GB / 47.38GB (31.6%) | RESERVED: 14.96GB\n",
      "\n",
      ">>> LOADING DATASET...\n",
      "\n",
      "ðŸ’¾ TRAINING CONFIG SAVED TO: holo_llm_checkpoints_20260118_131216/training_config.json\n",
      "\n",
      "================================================================================\n",
      "START TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING STARTED\n",
      "================================================================================\n",
      "START TIME: 2026-01-18 13:12:21\n",
      "LOG DIRECTORY: holo_llm_checkpoints_20260118_131216/logs\n",
      "INITIAL VRAM: 14.96GB / 47.38GB (31.6%) | RESERVED: 14.96GB\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7402/4085374055.py:117: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):  # Disable AMP for FFT\n",
      "/tmp/ipykernel_7402/4085374055.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 20:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.092600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 | LOSS: 1.8614 | LR: 1.87e-05\n",
      "[STEP 10] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.21GB\n",
      "STEP 10 | LOSS: 0.1116 | LR: 1.71e-05\n",
      "STEP 15 | LOSS: 0.1034 | LR: 1.56e-05\n",
      "[STEP 20] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.38GB\n",
      "STEP 20 | LOSS: 0.0995 | LR: 1.40e-05\n",
      "STEP 25 | LOSS: 0.0943 | LR: 1.24e-05\n",
      "[STEP 30] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.64GB\n",
      "STEP 30 | LOSS: 0.0926 | LR: 1.08e-05\n",
      "STEP 35 | LOSS: 0.0961 | LR: 9.21e-06\n",
      "[STEP 40] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.37GB\n",
      "STEP 40 | LOSS: 0.0948 | LR: 7.62e-06\n",
      "STEP 45 | LOSS: 0.0952 | LR: 6.03e-06\n",
      "[STEP 50] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.35GB\n",
      "STEP 50 | LOSS: 0.0971 | LR: 4.44e-06\n",
      "STEP 55 | LOSS: 0.0920 | LR: 2.86e-06\n",
      "[STEP 60] VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.46GB\n",
      "STEP 60 | LOSS: 0.0892 | LR: 1.27e-06\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETED\n",
      "================================================================================\n",
      "END TIME: 2026-01-18 13:33:22\n",
      "DURATION: 0:21:01.175862\n",
      "Final VRAM: 17.80GB / 47.38GB (37.6%) | RESERVED: 37.56GB\n",
      "\n",
      "TRAINING STATISTICS:\n",
      "=> AVERAGE LOSS: 0.2439\n",
      "=>MINIMUM LOSS: 0.0892\n",
      "=>FINAL LOSS: 0.0892\n",
      "\n",
      "LOGS SAVED TO: holo_llm_checkpoints_20260118_131216/logs\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ’¾ Saving the final model now: ./holo_llm_checkpoints_20260118_131216/final_model\n",
      "\n",
      "âœ… TRAINING SUCESSFULLY!\n",
      "ðŸ“Š LOGS: ./holo_llm_checkpoints_20260118_131216/logs\n",
      "ðŸ’¾ CHECKPOINTS: ./holo_llm_checkpoints_20260118_131216\n",
      "ðŸŽ¯ FINAL MODEL: ./holo_llm_checkpoints_20260118_131216/final_model\n"
     ]
    }
   ],
   "source": [
    "# Training Qwen with HoloAdam Optimizer\n",
    "holo_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77561c9-9787-4d7a-a590-17e307f465e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADAFACTOR MAIN TRAINING FUNCTION\n",
    "# ==============================================================================\n",
    "def adafactor_training():\n",
    "    # Auto configuration\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INIT TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    config = auto_configure_training()\n",
    "    \n",
    "    # Configure parameters\n",
    "    MODEL_ID = config[\"model_id\"]\n",
    "    BATCH_SIZE = config[\"per_device_train_batch_size\"]\n",
    "    GRADIENT_ACCUMULATION_STEPS = config[\"gradient_accumulation_steps\"]\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    LR = 2e-5\n",
    "    NUM_TRAIN_EPOCHS = 1\n",
    "    LOGGING_STEPS = 5\n",
    "    SAVE_STEPS = 100\n",
    "    \n",
    "    # Creating folder output with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_DIR = f\"./adafactor_llm_checkpoints_{timestamp}\"\n",
    "    LOG_DIR = f\"{OUTPUT_DIR}/logs\"\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“ OUTPUT DIRECTORY: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸ“ LOG DIRECTORY: {LOG_DIR}\")\n",
    "    \n",
    "    # Loading model and tokenizer\n",
    "    print(\"\\n>>> LOADING MODEL AND TOKENIZER...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=\"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\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        use_cache=False,\n",
    "        token=\"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\"\n",
    "    ).cuda()\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    VRAMMonitor.print_vram_status(prefix=\"After Model Load: \")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\n>>> LOADING DATASET...\")\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:2000]\")\n",
    "    \n",
    "    def fmt(x):\n",
    "        return {\"text\": f\"User: {x['instruction']}\\nAI: {x['output']}\"}\n",
    "    \n",
    "    dataset = dataset.map(fmt)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        outputs[\"labels\"] = [ids.copy() for ids in outputs[\"input_ids\"]]\n",
    "        return outputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Setup training arguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_dir=LOG_DIR,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        \n",
    "        save_strategy=\"no\",\n",
    "        # save_steps=SAVE_STEPS,\n",
    "        # save_total_limit=3,\n",
    "        \n",
    "        report_to=\"tensorboard\",\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "    \n",
    "    # Init callback\n",
    "    logging_callback = DetailedLoggingCallback(\n",
    "        log_dir=LOG_DIR,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    \n",
    "    # Init Adafactor trainer\n",
    "    trainer = AdafactorTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        callbacks=[logging_callback]\n",
    "    )\n",
    "    \n",
    "    # Saving config\n",
    "    config_file = Path(OUTPUT_DIR) / \"training_config.json\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump({\n",
    "            \"model_id\": MODEL_ID,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"learning_rate\": LR,\n",
    "            \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "            \"num_epochs\": NUM_TRAIN_EPOCHS,\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ TRAINING CONFIG SAVED TO: {config_file}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"START TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Savig model\n",
    "    final_model_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "    print(f\"\\nðŸ’¾ SAVING THE FINAL MODEL NOW: {final_model_dir}\")\n",
    "    trainer.save_model(final_model_dir)\n",
    "    tokenizer.save_pretrained(final_model_dir)\n",
    "    \n",
    "    print(\"\\nâœ… TRAINING SUCESSFULLY!\")\n",
    "    print(f\"ðŸ“Š LOGS: {LOG_DIR}\")\n",
    "    print(f\"ðŸ’¾ CHECKPOINTS: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸŽ¯ FINAL MODEL: {final_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d175efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INIT TRAINING PIPELINE\n",
      "================================================================================\n",
      ">>> DETECT GPU VRAM: 47.38 GB\n",
      ">>> ENABLING WORKSTATION MODE (L40S) - HIGH PERFORMANCE\n",
      "# MODEL: meta-llama/Meta-Llama-3-8B\n",
      "# BATCH SIZE PER DEVICE: 2\n",
      "# ACCUMULATION GRADIENT: 16\n",
      "# HOLO COMPRESSION RATIO: 64\n",
      "\n",
      "ðŸ“ OUTPUT DIRECTORY: ./adafactor_llm_checkpoints_20260121_171410\n",
      "ðŸ“ LOG DIRECTORY: ./adafactor_llm_checkpoints_20260121_171410/logs\n",
      "\n",
      ">>> LOADING MODEL AND TOKENIZER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4db6fdb702e4119949036fdc60ebc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dc1d033f3545d6845a1a14def5a7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Model Load: VRAM: 14.96GB / 47.38GB (31.6%) | RESERVED: 14.96GB\n",
      "\n",
      ">>> LOADING DATASET...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f706ed2034c54ff8b23521e850aa774c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc14fbcf7af42e8ac22bf793f212af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(â€¦):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a8a0431fca4fb2b11cf98e62c3aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a16884eef243bb9d156de85017293f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31a32c0aeae4f22bcbc661188fe3bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ TRAINING CONFIG SAVED TO: adafactor_llm_checkpoints_20260121_171410/training_config.json\n",
      "\n",
      "================================================================================\n",
      "START TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING STARTED\n",
      "================================================================================\n",
      "START TIME: 2026-01-21 17:14:16\n",
      "LOG DIRECTORY: adafactor_llm_checkpoints_20260121_171410/logs\n",
      "INITIAL VRAM: 14.96GB / 47.38GB (31.6%) | RESERVED: 14.96GB\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 15:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>7.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.374300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5 | LOSS: 7.4032 | LR: 1.87e-05\n",
      "[STEP 10] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 10 | LOSS: 7.4460 | LR: 1.71e-05\n",
      "STEP 15 | LOSS: 7.3793 | LR: 1.56e-05\n",
      "[STEP 20] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 20 | LOSS: 7.3487 | LR: 1.40e-05\n",
      "STEP 25 | LOSS: 7.3894 | LR: 1.24e-05\n",
      "[STEP 30] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 30 | LOSS: 7.4229 | LR: 1.08e-05\n",
      "STEP 35 | LOSS: 7.3947 | LR: 9.21e-06\n",
      "[STEP 40] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 40 | LOSS: 7.3929 | LR: 7.62e-06\n",
      "STEP 45 | LOSS: 7.3839 | LR: 6.03e-06\n",
      "[STEP 50] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 50 | LOSS: 7.3518 | LR: 4.44e-06\n",
      "STEP 55 | LOSS: 7.3463 | LR: 2.86e-06\n",
      "[STEP 60] VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "STEP 60 | LOSS: 7.3743 | LR: 1.27e-06\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETED\n",
      "================================================================================\n",
      "END TIME: 2026-01-21 17:30:08\n",
      "DURATION: 0:15:51.994936\n",
      "Final VRAM: 14.98GB / 47.38GB (31.6%) | RESERVED: 34.18GB\n",
      "\n",
      "TRAINING STATISTICS:\n",
      "=> AVERAGE LOSS: 7.3861\n",
      "=>MINIMUM LOSS: 7.3463\n",
      "=>FINAL LOSS: 7.3743\n",
      "\n",
      "LOGS SAVED TO: adafactor_llm_checkpoints_20260121_171410/logs\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ’¾ SAVING THE FINAL MODEL NOW: ./adafactor_llm_checkpoints_20260121_171410/final_model\n",
      "\n",
      "âœ… TRAINING SUCESSFULLY!\n",
      "ðŸ“Š LOGS: ./adafactor_llm_checkpoints_20260121_171410/logs\n",
      "ðŸ’¾ CHECKPOINTS: ./adafactor_llm_checkpoints_20260121_171410\n",
      "ðŸŽ¯ FINAL MODEL: ./adafactor_llm_checkpoints_20260121_171410/final_model\n"
     ]
    }
   ],
   "source": [
    "# Training Qwen with Adafactor Optimizer\n",
    "adafactor_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58a661-3916-46c4-bdf3-a09270154c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
